# Text2Image Server Configuration
# Modify these settings to configure the server

# ============================================================================
# Concurrency Settings
# ============================================================================
concurrency:
  # Maximum concurrent image generations (adjust based on GPU VRAM)
  # Recommended: 1-2 for lower VRAM usage, 3-4 for GPUs with 24GB+ VRAM
  max_concurrent: 2
  
  # Maximum number of requests in queue
  max_queue: 50
  
  # Request timeout in seconds (5 minutes default)
  request_timeout: 300

# ============================================================================
# Model Settings - OPTIMIZED FOR MAXIMUM SPEED
# ============================================================================
model:
  # Model name from HuggingFace
  name: "Tongyi-MAI/Z-Image-Turbo"
  # name: "Disty0/Z-Image-Turbo-SDNQ-uint4-svd-r32"
  
  # Optional: Explicit path to local model directory
  # If set, will use this path directly. If None/empty, will auto-detect local models.
  # Auto-detection checks: HuggingFace cache, current directory, ./models/
  # Example: local_path: "./models--Tongyi-MAI--Z-Image-Turbo"
  # Example: local_path: "C:/Users/username/.cache/huggingface/hub/models--Tongyi-MAI--Z-Image-Turbo"
  local_path: null
  
  # Torch dtype: "bfloat16", "float16", or "float32"
  # bfloat16 is recommended for modern GPUs (fastest)
  torch_dtype: "bfloat16" 
  
  # Enable modern torch.compile() (PyTorch 2.0+) - MUCH faster than legacy compilation
  # This provides significant speedup after first run (compilation happens on warmup)
  # Options: "default", "reduce-overhead", "max-autotune"
  # "reduce-overhead" is recommended for best speed/compilation time balance
  enable_torch_compile: false
  torch_compile_mode: "reduce-overhead"
  
  # Legacy model compilation (fallback if torch.compile fails)
  # NOTE: Requires Triton to be installed (pip install triton)
  enable_compilation: true
  
  # Enable CPU offloading (saves GPU memory, SLOWER inference)
  # Disable for maximum speed - only enable if you run out of VRAM
  enable_cpu_offload: false
  
  # Enable sequential CPU offloading (most memory efficient, MUCH SLOWER)
  # Disable for maximum speed - only enable if you run out of VRAM
  enable_sequential_cpu_offload: false
  
  # Enable VAE slicing (reduces VRAM usage, SLOWS DOWN inference)
  # DISABLED for speed - use VAE tiling instead if VRAM is limited
  enable_vae_slicing: false
  
  # Enable VAE tiling (memory efficient for large images, minimal speed impact)
  # Better than slicing - enables efficient processing of large images
  enable_vae_tiling: false
    
  # Enable attention slicing (reduces VRAM usage, SLOWS DOWN inference)
  # DISABLED for speed - only enable if you run out of VRAM
  enable_attention_slicing: false
  
  # Use low CPU memory usage when loading model
  # Disabled for faster model loading (if you have enough RAM)
  low_cpu_mem_usage: false
  
  # Enable Flash Attention (MUCH faster attention computation)
  # Automatically uses Flash Attention 3 if available, falls back to 2
  enable_flash_attention: true
  
  # Enable optimized VAE decoding (faster VAE processing)
  enable_optimized_vae: true
  
  # CUDA graphs for repeated inference patterns (experimental)
  # Can provide additional speedup for identical request patterns
  enable_cuda_graphs: false

# ============================================================================
# Storage Settings
# ============================================================================
storage:
  # Directory to save generated images (relative to script location)
  images_dir: "images"
  
  # Whether to save images to disk
  # Set to true to save, false to only return base64
  save_images: false

