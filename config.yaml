# Text2Image Server Configuration
# Modify these settings to configure the server

# ============================================================================
# Concurrency Settings
# ============================================================================
concurrency:
  # Maximum concurrent image generations (adjust based on GPU VRAM)
  # Recommended: 1-2 for lower VRAM usage, 3-4 for GPUs with 24GB+ VRAM
  max_concurrent: 2
  
  # Maximum number of requests in queue
  max_queue: 50
  
  # Request timeout in seconds (5 minutes default)
  request_timeout: 300

# ============================================================================
# Model Settings
# ============================================================================
model:
  # Model name from HuggingFace
  # name: "Tongyi-MAI/Z-Image-Turbo"
  name: "Disty0/Z-Image-Turbo-SDNQ-uint4-svd-r32"
  
  # Torch dtype: "bfloat16", "float16", or "float32"
  # bfloat16 is recommended for modern GPUs
  torch_dtype: "bfloat16"
  
  # Enable model compilation (faster inference, slower first run)
  # NOTE: Requires Triton to be installed (pip install triton)
  # If Triton is not available, compilation will be automatically disabled
  enable_compilation: false
  
  # Enable CPU offloading (saves GPU memory, slower inference)
  # Recommended for GPUs with < 12GB VRAM
  enable_cpu_offload: false
  
  # Enable sequential CPU offloading (most memory efficient, slower inference)
  # Moves model components to CPU when not in use
  # Recommended for GPUs with < 8GB VRAM
  enable_sequential_cpu_offload: false
  
  # Enable VAE slicing (reduces VRAM usage for VAE decoder)
  # Minimal performance impact, recommended to keep enabled
  enable_vae_slicing: true
  
  # Enable attention slicing (reduces VRAM usage for attention layers)
  # Minimal performance impact, recommended to keep enabled
  enable_attention_slicing: true
  
  # Use low CPU memory usage when loading model
  # Reduces peak CPU memory during model loading
  low_cpu_mem_usage: false
  
  # Enable Flash Attention (faster attention computation)
  # Requires compatible GPU and CUDA version
  enable_flash_attention: true

# ============================================================================
# Storage Settings
# ============================================================================
storage:
  # Directory to save generated images (relative to script location)
  images_dir: "images"
  
  # Whether to save images to disk
  # Set to true to save, false to only return base64
  save_images: false

